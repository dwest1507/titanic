---
title: "EDA of Titanic Data"
author: "David West"
date: "8/13/2020"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Initial set up of libraries
source(here::here("src","_init_.R"))

# Loading the data set
source(here::here("src","data","make_dataset.R"))

# Feature engineering
source(here::here("src","features","build_features.R"))
```

# Introduction
The purpose of this exploratory analysis is to test out different machine learning algorithms.

# Ground Rules and Assumptions
1. There are NAs for Age in the training data set. A median age will be assumend for all Age NAs. <br />
2. There is one NA for Far in the data. A median Fare will be assumed. <br />
3. PassengerID, Name, Ticket, and Cabin will be removed from the training data set since there are too many levels for some ML algorithms. May have to come back to this and find a way to incorporate its data still.

```{r}
train2 <- train %>% 
  select(-c(Name, PassengerId, Ticket, Cabin))
```


# Models
## randomForest
### Attempt 1
A randomForest model is created below.
```{r}
set.seed(123)
rf_model1 <- randomForest(Survived ~ ., data = train2)

print(rf_model1)
```
This is a `r rf_model1$type` random forest model. `r rf_model1$ntree` trees are used. `r rf_model1$mtry` variables are tried at each split in a tree, which is (by default) calculated from the square root of total features. The out of bag (OOB) error rate is `r as.numeric(rf_model1$err.rate[nrow(rf_model1$err.rate),"OOB"])`. A random forest model uses bootstrap sampling. This means for each tree, observations are randomly pulled from the data N [nrow(data)] amount of times. An observation has the potential of being duplicated or absent from a bootstrap sample. The absent observations are called the OOB set or OOB sample. Since the OOB sample was not used to train the random forest, it can be used to test the performance of the model. The error across all the OOB samples is called the OOB error (in this case, `r as.numeric(rf_model1$err.rate[nrow(rf_model1$err.rate),"OOB"])`). The OOB matrix is displayed below. Each row represents one tree in this model.
```{r}
err <- rf_model1$err.rate

head(err)
```
The last row in this matrix is the final OOB error. This is the same value that is printed in the model output, "OOB estimate of  error rate".
```{r}
oob_err <- err[nrow(err),"OOB"]

print(oob_err)
```
When the random forest model is plotted, it shows the OOB error as a function of the number of trees in the forest. This can be used to determine a point of diminishing returns for number of trees.

```{r}
plot(rf_model1)

legend(x="right",
       legend = colnames(err),
       fill = 1:ncol(err))
```
It appears that the OOB error stays relatively flat in between 200 and 300 trees. Therefore, only ~300 trees are necessary for this model. There's nothing wrong with using "too many" trees. However computing predictions for each tree does take time. So if this was a bigger data set and computing speed was too slow, I would recommend cutting the model down to ~300 trees. Since this is a smaller data set. I will leave it as is. Time to generate predictions from this model.

```{r}
rf_model1_predictions <- predict(object = rf_model1,
                                newdata = test,
                                type = "class")

rf_model1_submission <- test %>% 
  as_tibble() %>% 
  mutate(Survived = rf_model1_predictions) %>% 
  select(PassengerId, Survived)
```
```{r}
# write.csv(rf_model1_submission,
#           here::here("data","processed","rf_model1_submission.csv"),
#           row.names = F)
```
This model was uploaded to Kaggle and received a score of 0.74401. This means my predictions were 74.401% accurate. This put me at the rank of 17,048 out of 19,132. However, my rank is in the top 90th percentile. There must be a lot of duplicate scores above me in rank. Not too bad for a first try! The script below confirms this percent.
```{r}
test %>% 
  mutate(Pred = rf_model1_predictions,
         Correct = Pred==Survived) %>% 
  summarise(PrecentCorrect = mean(Correct))
```


### Attempt 2
This model can be tuned by the following hyperparameters: <br />
1. ntree: number of trees <br />
2. mtry: number of variables randomly sampled as candidates at each split <br />
3. sampsize: number of samples to train on <br />
4. nodesize: minimum size (number of samples) of the terminal nodes <br />
5. maxnodes: maximum number of terminal nodes <br />
<br />
First, I'll use the tuneRF function and see which mtry yields the lowest OOB error.
```{r}
res <- tuneRF(x = subset(train2, select = -Survived),
              y = train2$Survived,
              ntreeTry = 500,
              doBest = T)

print(res)
```
Looks like 3 is the way to go. The original model had an mtry of three, so this doesn't help much. Next, I'll establish a list of possible values for mtry, nodesize and sampsize. BEWARE! The below script takes awhile to run. 
```{r}
# mtry <- seq(2,ncol(train2)*0.8,1)
# nodesize <- seq(3,8,1)
# sampsize <- round(nrow(train2) * seq(0.7,0.8,0.01), digits = 0)
# 
# hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)
# 
# oob_err <- c()
# 
# for (i in 1:nrow(hyper_grid)){
#   
#   model <- randomForest(Survived ~ ., 
#                         data = train2,
#                         mtry = hyper_grid$mtry[i],
#                         nodesize = hyper_grid$nodesize[i],
#                         sampsize = hyper_grid$sampsize[i])
#   
#   oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
# }
# 
# optimal <- which.min(oob_err)
# print(hyper_grid[optimal,])
```
Looks like a mtry of 4, nodesize of 7, and sampsize of 624 may be the best route. Let's look at this model.
```{r}
set.seed(123)
rf_model2 <- randomForest(Survived ~ .,
                          data = train2,
                          mtry = 4,
                          nodesize = 7,
                          sampsize = 624)

print(rf_model2)
```
It's still not as good as the first model. It's OOB error is `r as.numeric(rf_model2$err.rate[nrow(rf_model2$err.rate),"OOB"])`. The first model has an OOB error of `r as.numeric(rf_model1$err.rate[nrow(rf_model1$err.rate),"OOB"])`. I think this random forest isn't getting any better through tuning.

## nnet


## svm
## glmnet
## Comparing Models
### AUC
Each model can be compared against each other using the metric area under the curve (AUC). The AUC for the first random forest model is below:
```{r}
pred <- predict(object = rf_model1,
                newdata = test,
                type = "prob")

auc(actual = test$Survived,
    predicted = pred[,"1"])
```


